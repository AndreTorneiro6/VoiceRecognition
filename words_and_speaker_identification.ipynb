{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a048f3e2",
   "metadata": {},
   "source": [
    "# This script is to identify words by the phonetic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ecfccf",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "Run the next cell only if you don't have the packages already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scipy matplotlib sounddevice wave playsound tqdm librosa pyphen opencv-python pandas parselmouth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd801101",
   "metadata": {},
   "source": [
    "To add sugestions and auto complete in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c277256",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter_contrib_nbextensions\n",
    "!pip install jupyter_nbextensions_configurator\n",
    "!jupyter contrib nbextension install --user \n",
    "!jupyter nbextensions_configurator enable --user\n",
    "# after packages are installed\n",
    "#  - go to home page\n",
    "#  - click on nbextensions tab\n",
    "#  - unckeck disable configuration for nbextensions without explicit compatibility\n",
    "#  - put a check on Hinterland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dca56b",
   "metadata": {},
   "source": [
    "## <center><span style='color :blue' >Create dataset</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from os.path import join, exists\n",
    "import sounddevice as mic\n",
    "from scipy.io.wavfile import write\n",
    "import os\n",
    "\n",
    "\n",
    "def create_directories():\n",
    "    # create folder that contain recorded words\n",
    "    folder_name = 'Dataset'\n",
    "\n",
    "    # create subfolders with the name of each group member\n",
    "    sub_folders = ['Andre', 'Marcelo', 'Jorge']\n",
    "    \n",
    "    # create subsubfolders with the word\n",
    "    sub_sub_folders = ['chata','chapa','chave','lata','lapa','lava','casa','capa','cave','chuta','chupa',\n",
    "                       'chuva','farta','farpa','farda','ripa','rita','rica']\n",
    "    \n",
    "    principal_directory = join(os.getcwd(), folder_name)\n",
    "\n",
    "    # check if the principal folder already exist\n",
    "    if not exists(principal_directory):\n",
    "        os.mkdir(principal_directory)\n",
    "        \n",
    "    # check if the sub folder already exist\n",
    "    for sub_folder in sub_folders:\n",
    "        if not exists(join(principal_directory, sub_folder)):\n",
    "            os.mkdir(join(principal_directory, sub_folder))\n",
    "            \n",
    "        # check if the sub sub folder already exist\n",
    "        for sub_sub_folder in sub_sub_folders:\n",
    "            if not exists(join(principal_directory, sub_folder, sub_sub_folder)):\n",
    "                os.mkdir(join(principal_directory, sub_folder, sub_sub_folder))\n",
    "                \n",
    "\n",
    "def record_audio():\n",
    "    # get the word and the person name by receive input from keyboard\n",
    "    person_name = input('Please enter your name:\\n')\n",
    "    audio_name = input('Please enter the word that you will say:\\n')\n",
    "    print( f'{person_name} will say the word {audio_name}')\n",
    "    \n",
    "    # directory to save the audio recorded\n",
    "    save_name = join(os.getcwd(),'Dataset',person_name) \n",
    "    if not exists(save_name): # check if the folder exist\n",
    "        print(\"\"\"The possible names are:\n",
    "        - Andre \n",
    "        - Marcelo \n",
    "        - Jorge \"\"\")\n",
    "        record_audio() # try record audio again\n",
    "    \n",
    "\n",
    "    fs = 44100  # Sample rate of the audio file\n",
    "    seconds = 3  # Duration of recording\n",
    "    \n",
    "    print('Please say the word')\n",
    "    myrecording = mic.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "    mic.wait()  # Wait until recording is finished\n",
    "    write(join(save_name , (audio_name + '.wav')), fs, myrecording)  # Save as WAV file \n",
    "    print(f'{audio_name} is saved in {person_name} folder')\n",
    "# record_audio()  \n",
    "create_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623d30a",
   "metadata": {},
   "source": [
    "## <center> <span style='color :blue' >Sound Analysis</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c65cf",
   "metadata": {},
   "source": [
    "## Import all required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ce701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import all the required libraries in code below\n",
    "from librosa import load, get_duration, display, feature, magphase, frames_to_time, onset, autocorrelate, effects, stft, amplitude_to_db, times_like, pyin\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score, classification_report\n",
    "from scipy.signal import lfilter, lfilter_zi, filtfilt, butter, freqz, spectrogram, get_window, find_peaks\n",
    "import pyphen, math, time, glob, os, cv2, pickle, parselmouth\n",
    "from sklearn.model_selection import train_test_split\n",
    "from librosa.util import fix_length\n",
    "from parselmouth.praat import call\n",
    "# from pydub import AudioSegments\n",
    "from os.path import join, exists\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from sklearn.svm import SVC\n",
    "import scipy.fftpack as fft\n",
    "from math import log as ln\n",
    "from tqdm import tqdm\n",
    "from math import exp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841d3ad",
   "metadata": {},
   "source": [
    "### Read and View the graph of each audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fd877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_audios():\n",
    "    words_examples = [] # list initialization\n",
    "    audio_files = glob.glob('Dataset/*')  # get all folders inside Dataset folder\n",
    "    \n",
    "    for file in audio_files:\n",
    "        words = glob.glob(f'{file}/*') # get all examples inside each person folder    \n",
    "        for word in words:\n",
    "            examples = glob.glob(f'{word}/*') # get all examples inside each person folder    \n",
    "            if len(examples): # check if exist any example in each person\n",
    "                for example in examples:\n",
    "                    if example.endswith('.wav'):\n",
    "                        words_examples.append(example) # append all examples in list\n",
    "\n",
    "    return words_examples \n",
    "\n",
    "def read_audio_signal(audio_file):    \n",
    "    audio, fs = load(audio_file) # read audio file with fs = 22050 in mono mode\n",
    "\n",
    "    padded_audio = fix_length(audio, size=4*fs)\n",
    "    duration = get_duration(y=padded_audio, sr=fs) # get the duration of the audio in seconds\n",
    "    \n",
    "    duration_time = np.linspace(0, duration, audio.shape[0]) # create time vector to x-scale in graph\n",
    "\n",
    "    number_points = int(fs * duration) # signal size in points\n",
    "    \n",
    "    return fs, padded_audio, duration_time, number_points\n",
    "        \n",
    "def show_orginal_audios(): # show original wave of each word\n",
    "    start_time = time.time()\n",
    "    audio_examples = get_audios() # call function to get all audios in dataset\n",
    "    size =  round(np.sqrt(len(audio_examples)))\n",
    "    for i in tqdm(range(len(audio_examples))):\n",
    "        print(i)\n",
    "        fs, audio_readed, duration_time, _ = read_audio_signal(audio_examples[i]) # read audio example\n",
    "        \n",
    "        # define ghapics parameters \n",
    "        plt.subplot(size, size, i+1)\n",
    "        plt.rcParams['figure.figsize'] = [15, 7]\n",
    "        plt.tight_layout(pad=3.0)\n",
    "\n",
    "        # show graph with original audio signal\n",
    "        plt.plot(duration_time,audio_readed)\n",
    "        # plt.xlabel('Time [s]')\n",
    "        # plt.ylabel('Amplitude')\n",
    "        # plt.title(audio_examples[i].split('\\\\')[1] + ' saying ' +  audio_examples[i].split('\\\\')[-1].split('.')[0])\n",
    "    plt.show()   \n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    print(elapsed_time)\n",
    "    \n",
    "# show_orginal_audios()   \n",
    "# get_audios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a900679",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, fs = load('Dataset/Andre/casa/0.wav') # read audio file with fs = 22050 in mono mode\n",
    "y, sr = librosa.load('Dataset/Andre/casa/0.wav')\n",
    "librosa.get_duration(y=y, sr=sr)\n",
    "padded_audio = fix_length(audio, size=4*fs)\n",
    "duration = librosa.get_duration(y=padded_audio,sr=fs) # get the duration of the audio in seconds\n",
    "display.waveshow(audio, sr=fs)\n",
    "\n",
    "display.waveshow(padded_audio, sr=fs, alpha = 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d527ea",
   "metadata": {},
   "source": [
    "## Filter the original signal between typical voice frequencies\n",
    "\n",
    "- The most common voice frequencies are between 60 Hz to 7000 Hz \"https://www.atlasdasaude.pt/publico/content/voz\";\n",
    "\n",
    "\n",
    "- Due to voice has a low and high range will be aplied a band-pass filter;\n",
    "\n",
    "\n",
    "- The filter will be applied forward and backward to avoid delays in the filtered signal when compared with the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152df2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_original_voice(filter_response = False, visualize_graph = False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    filtered_audios = [] # initialize list\n",
    "    \n",
    "    audio_examples = get_audios() # call function to get all audios in dataset\n",
    "    size =  round(np.sqrt(len(audio_examples)))\n",
    "    for i in tqdm(range(len(audio_examples))):\n",
    "        fs, audio_readed, duration_time, _ = read_audio_signal(audio_examples[i]) # read audio example\n",
    "        \n",
    "        nyq = 0.5 * fs # nyquist frequency\n",
    "        \n",
    "        # normalize cutoff frequency\n",
    "        low = 60 / nyq \n",
    "        high = 7000 / nyq \n",
    "        \n",
    "        b, a = butter(6, [low, high], btype='band') # define 5th order band-pass butterworth filter \n",
    "\n",
    "        w, h = freqz(b, a) # compute frequency response\n",
    "\n",
    "        zi = lfilter_zi(b, a) # set initial state of the filter\n",
    "        z, _ = lfilter(b, a, audio_readed, zi=zi*audio_readed[0]) # filter signal from left to right\n",
    "        z2, _ = lfilter(b, a, z, zi=zi*z[0]) # filter signal from right to left\n",
    "        audio_filtered = filtfilt(b, a, audio_readed) # filter signal forward and backward\n",
    "        \n",
    "        filtered_audios.append([audio_examples[i], audio_filtered, fs]) # save name and filtered audio in list\n",
    "        size =  round(np.sqrt(len(filtered_audios))) # get the sqrt of the list size to use in graphics parameters\n",
    "        if visualize_graph:\n",
    "            \n",
    "            # define graphics parameters \n",
    "            plt.subplot(size, size, i+1)\n",
    "            plt.rcParams['figure.figsize'] = [15, 7]\n",
    "            plt.tight_layout(pad=3.0)\n",
    "\n",
    "            # show graph with original audio signal\n",
    "            plt.plot(duration_time,audio_readed, 'r', linewidth = 3)\n",
    "\n",
    "            # show graph with filtered audio signal\n",
    "            plt.plot(duration_time,audio_filtered, 'g', alpha = 0.8)\n",
    "            plt.legend(('original signal','Filtered signal'), loc = 3, prop={'size': 7})  \n",
    "            plt.grid(True)\n",
    "            plt.xlabel('Time [s]')\n",
    "            plt.ylabel('Amplitude')\n",
    "            plt.title(audio_examples[i].split('\\\\')[1] + ' saying ' +  audio_examples[i].split('\\\\')[-1].split('.')[0])\n",
    "            \n",
    "    if visualize_graph:\n",
    "        plt.show()\n",
    "    \n",
    "    # show frequency response\n",
    "    if filter_response:\n",
    "        plt.plot((fs * 0.5 / np.pi) * w, 20 * np.log10(abs(h)), 'b')\n",
    "        plt.ylabel('Amplitude [dB]', color='b')\n",
    "        plt.xlabel('Frequency [rad/sample]')   \n",
    "        plt.title('Frequency Response')  \n",
    "        plt.grid(True)\n",
    "        plt.xticks(range(0, 12000, 1000))\n",
    "        plt.ylim(-500, 10)\n",
    "\n",
    "        # Clearer view of frequency response at high cutoff frequency\n",
    "        ax1 =  plt.axes([0.2, 0.3, 0.2, 0.2])\n",
    "        ax1.plot((fs * 0.5 / np.pi) * w, 20 * np.log10(abs(h)), 'r')\n",
    "        plt.grid(True)\n",
    "        plt.title('Zoom in high cutoff')   \n",
    "        plt.xlim(5000,8000)\n",
    "        plt.ylim(-10, 10)\n",
    "        \n",
    "        # Clearer view of frequency response at low cutoff frequency\n",
    "        ax2 =  plt.axes([0.6, 0.3, 0.2, 0.2])\n",
    "        ax2.plot((fs * 0.5 / np.pi) * w, 20 * np.log10(abs(h)), 'r')\n",
    "        plt.grid(True)\n",
    "        plt.title('Zoom in low cutoff')   \n",
    "        plt.xlim(0,100)\n",
    "        plt.ylim(-200, 10)\n",
    "\n",
    "        plt.show()\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    print('All audios are filtered')\n",
    "    return filtered_audios, size\n",
    "\n",
    "\n",
    "# call function to visualize frequency response and graphic of each filtered audio\n",
    "filtered_audios, _ = filter_original_voice(filter_response= False, visualize_graph=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7df40",
   "metadata": {},
   "source": [
    "### View spectrogram to see where are the power and silence in filtered signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "def see_spectrogram_scipy():\n",
    "    \n",
    "    filtered_audios = filter_original_voice(filter_response = False, visualize_graph = False) # call function to get all filtered audios\n",
    "    for i in range(len(filtered_audios)):\n",
    "        \n",
    "        name = filtered_audios[i][0] # get the word and the person\n",
    "        wave = filtered_audios[i][1] # get audio wave\n",
    "        fs = filtered_audios[i][2] # get sample rate\n",
    "        \n",
    "        # define ghapics parameters \n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.rcParams['figure.figsize'] = [15, 15]\n",
    "        plt.tight_layout(pad=3.0)\n",
    "\n",
    "        # perfom spectogram analysis\n",
    "        f, t, S1 = spectrogram(wave,fs,window='flattop', nperseg=fs//10, noverlap=fs//20, scaling='density', mode='magnitude')\n",
    "        \n",
    "        # show graph with spectogram of each audio\n",
    "        plt.pcolormesh(t, f[:800], S1[:800][:])\n",
    "        plt.xlabel('time(s)')\n",
    "        plt.ylabel('frequency(Hz)')\n",
    "        plt.title(name.split('\\\\')[1]+ ' saying ' + name.split('\\\\')[-1].split('.')[0])\n",
    "        \n",
    "    plt.show()\n",
    "     \n",
    "        \n",
    "def see_spectrogram_librosa (y_scale = 'log'):\n",
    "    filtered_audios, size = filter_original_voice(filter_response = False, visualize_graph = False) # call function to get all filtered audios\n",
    "    for i in tqdm(range(len(filtered_audios))):\n",
    "        \n",
    "        name = filtered_audios[i][0] # get the word and the person\n",
    "        wave = filtered_audios[i][1] # get audio wave\n",
    "        fs = filtered_audios[i][2] # get sample rate\n",
    "        \n",
    "        \n",
    "        X = stft(wave) # aply stft to filtered signal\n",
    "        Xdb = amplitude_to_db(abs(X)) # convert signal only to positive values and convert amplitude to db\n",
    "        \n",
    "        # define ghap parameters \n",
    "        plt.subplot(size, size, i+1)\n",
    "        plt.rcParams['figure.figsize'] = [15, 7]\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        \n",
    "        # show graph with spectogram of each audio\n",
    "        librosa.display.specshow(Xdb[:800], sr=fs, x_axis='time', y_axis= y_scale)\n",
    "        \n",
    "    plt.colorbar()\n",
    "    \n",
    "see_spectrogram_librosa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9e9f4",
   "metadata": {},
   "source": [
    "### Split the audio in silence and non-silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_consecutive(lst, distance = 3):\n",
    "    \n",
    "    # for loop to iterate the list   \n",
    "    for i,j in enumerate(lst,lst[0]): \n",
    "        \n",
    "        # get the non consecutive values which represent words in the audio\n",
    "        if i!=j and j - i > distance: \n",
    "            return [i,j], j\n",
    "                \n",
    "\n",
    "def calculate_rms(show_graph = False):  \n",
    "    \n",
    "    get_exp_value = False # bool variable to indicate if exponential value is already known\n",
    "    audio_flag= []\n",
    "    frame_length = 2048\n",
    "    hop_length = 1024\n",
    "    \n",
    "    for i in tqdm(range(len(filtered_audios))):\n",
    "        \n",
    "        name = filtered_audios[i][0] # get the word and the person\n",
    "        wave = filtered_audios[i][1] # get audio wave\n",
    "        fs = filtered_audios[i][2] # get sample rate    \n",
    "        \n",
    "#         print(name, i)\n",
    "        # determine average power in each frame\n",
    "        rms = feature.rms(y=abs(wave), frame_length=frame_length, hop_length=hop_length, center=True)[0]\n",
    "        \n",
    "        rms_rounded = np.round(rms, decimals=2) # round rms values in 2 decimals\n",
    "        \n",
    "        if not get_exp_value:\n",
    "            difference_value = (len(wave)/len(rms_rounded))\n",
    "            x = math.floor(math.log10(difference_value))  # x = 3\n",
    "            exp_value = 1 * 10**x                       \n",
    "            get_exp_value = True\n",
    "            \n",
    "        # list to return values where audio is\n",
    "        silence_time = []\n",
    "        for rms_index in range(len(rms_rounded)):\n",
    "                if rms_rounded[rms_index] == 0:\n",
    "                    silence_time.append(rms_index)\n",
    "\n",
    "        marker_phrase, index = find_non_consecutive(silence_time) # get the limits of phrase in audio segment    \n",
    "        marker_words_limit = silence_time.index(index) # find the index of the limit in marker phrase\n",
    "        \n",
    "        del silence_time[:marker_words_limit] # delete all values in list until the limit to research\n",
    "        revaluate_word_position, _ = find_non_consecutive(silence_time) # get the limits of word in audio segment\n",
    "\n",
    "        phrase_begin = np.multiply(np.min(marker_phrase), exp_value) # get the value where phrase begin\n",
    "        phrase_end = np.multiply(np.max(marker_phrase), exp_value) # get the value where phrase end\n",
    "        \n",
    "        word_begin = np.multiply(np.min(revaluate_word_position), exp_value) # get the value where word begin     \n",
    "        word_end = np.multiply(np.max(revaluate_word_position),exp_value) # get the value where word end\n",
    "\n",
    "        audio_flag.append([phrase_begin,phrase_end,word_begin,word_end])    \n",
    "        \n",
    "        if show_graph:\n",
    "            # get number of points in signal and the respective time\n",
    "            frames_f = range(len(rms_rounded))\n",
    "            time_scale = frames_to_time(frames_f, sr=fs, hop_length=hop_length)\n",
    "\n",
    "            # define ghap parameters \n",
    "            plt.subplot(size, size, i+1)\n",
    "            plt.rcParams['figure.figsize'] = [15, 15]\n",
    "            plt.tight_layout(pad=3.0)\n",
    "\n",
    "            # show graph with rms of each audio\n",
    "            plt.plot(time_scale, rms_rounded, 'g')\n",
    "            plt.title(name.split('\\\\')[1]+ ' saying ' + name.split('\\\\')[-1].split('.')[0])\n",
    "    \n",
    "    if show_graph:\n",
    "        plt.show()\n",
    "    \n",
    "    print('End of rms calculation')\n",
    "    return audio_flag\n",
    "\n",
    "audio_flag = calculate_rms()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876f5d7",
   "metadata": {},
   "source": [
    "## Extract number of syllables in audio\n",
    "\n",
    "- To count the number of syllables in the word only the audio segment that contains the word will be used;\n",
    "\n",
    "\n",
    "- To implemenent this counter will be used *__\"OnSet detection\"__* from librosa;\n",
    "\n",
    "\n",
    "- The  *__\"OnSet detection\"__* method is used to find the moment where musical notes begin. As our voice is composed of different frequencies, and those frequencies can be atributed to musical notes, this method will help to find when each syllable begin; \"https://repositorio.iscte-iul.pt/handle/10071/5991\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20de1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_syllables():\n",
    "        \n",
    "    splited_phonemes = []\n",
    "    for i in tqdm(range(len(filtered_audios))): \n",
    "\n",
    "        name = filtered_audios[i][0] # get the word and the person\n",
    "        wave = filtered_audios[i][1] # get audio wave\n",
    "        fs = filtered_audios[i][2] # get sample rate \n",
    "        y = wave[audio_flag[i][2]:audio_flag[i][3]] # get only the\n",
    "\n",
    "        odf = onset.onset_strength(y=y, sr=fs, hop_length=512)\n",
    "        ac = autocorrelate(odf/odf.max(), max_size= fs // 512)\n",
    "        peaks, _ = find_peaks(ac, height=0.15)\n",
    "    #     print(len(ac),  name, i)\n",
    "\n",
    "        if len(peaks):\n",
    "            syllable_limit =  audio_flag[i][2] + math.floor(peaks[-1]/2) * 1000 + 1000\n",
    "        else:\n",
    "            syllable_limit =  audio_flag[i][2] + math.floor((len(y)/1000)/2) * 1000 + 1000\n",
    "        \n",
    "        splited_phonemes.append([audio_flag[i][2], syllable_limit, audio_flag[i][3], name])\n",
    "\n",
    "    print('All words are splited in phonemes')\n",
    "    \n",
    "    return splited_phonemes\n",
    "\n",
    "splited_phonemes = split_audio_syllables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963a2d0",
   "metadata": {},
   "source": [
    "### Put the values of splited audios in csv file\n",
    "This is only to facilitate the interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c75fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa import frames_to_time\n",
    "from librosa import feature\n",
    "import pandas as pd\n",
    "\n",
    "frame_length = 2048\n",
    "hop_length = 1024\n",
    "\n",
    "part1 = []\n",
    "part2 = []\n",
    "part3 = []\n",
    "part4 = []\n",
    "names = []\n",
    "\n",
    "for i in tqdm(range(len(filtered_audios))):\n",
    "    \n",
    "    name = filtered_audios[i][0] # get the word and the person\n",
    "    wave = filtered_audios[i][1] # get audio wave\n",
    "    fs = filtered_audios[i][2] # get sample rate    \n",
    "    \n",
    "    # determine average power in each frame\n",
    "    rms = feature.rms(abs(wave), frame_length=frame_length, hop_length=hop_length, center=True)[0]\n",
    "\n",
    "    rms_rounded = np.round(rms, decimals=2) # round rms values in 2 decimals\n",
    "    \n",
    "    difference_value = (len(wave)/len(rms_rounded))\n",
    "    x = math.floor(math.log10(difference_value))  # x = 3\n",
    "    exp_value = 1 * 10**x    \n",
    "    \n",
    "    silence_time = []\n",
    "    for rms_index in range(len(rms_rounded)):\n",
    "            if rms_rounded[rms_index] == 0:\n",
    "                silence_time.append(rms_index)\n",
    "            \n",
    "    marker_phrase, index = find_non_consecutive(silence_time) # get the limits of phrase in audio segment    \n",
    "    marker_words_limit = silence_time.index(index) # find the index of the limit of marker phrase\n",
    "    del silence_time[:marker_words_limit] # delete all values in list until the limit to research\n",
    "    revaluate_word_position, _ = find_non_consecutive(silence_time) # get the limits of word in audio segment\n",
    "\n",
    "    phrase_begin = np.multiply(np.min(marker_phrase), exp_value) # get the value where phrase begin\n",
    "    phrase_end = np.multiply(np.max(marker_phrase), exp_value) # get the value where phrase end\n",
    "\n",
    "    word_begin = np.multiply(np.min(revaluate_word_position), exp_value) # get the value where word begin     \n",
    "    word_end = np.multiply(np.max(revaluate_word_position),exp_value) # get the value where word end\n",
    "\n",
    "    \n",
    "    names.append(name)\n",
    "    part1.append(phrase_begin)\n",
    "    part2.append(phrase_end)\n",
    "    part3.append(word_begin)\n",
    "    part4.append(word_end)\n",
    "    \n",
    "df = pd.DataFrame(np.column_stack([names, part1, part2,part3,part4]), \n",
    "                               columns=['voiceID', 'limit1', 'limit2', 'limit3', 'limit4']) \n",
    "# Write out the updated dataframe\n",
    "df.to_csv(\"processed_results.csv\", index=False)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b51f2",
   "metadata": {},
   "source": [
    "#### Normalize audio values after pre-processement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(audio):\n",
    "    positive_audio = abs(audio) # convert audio to absolute values\n",
    "    audio = positive_audio / np.max(positive_audio) # divide audio values by max value in audio to normalize\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e915c5b",
   "metadata": {},
   "source": [
    "#### Aply framimg in audio\n",
    "- This process will help to avoid distortion in the FFT. \n",
    "\n",
    "\n",
    "Due to the audio being a non-stationary signal this will produce FFT distortions, so if we consider that audio is a stationary signal for short periods we will be capable to avoid those distortions. To implement this assumption, the signal will be divided into short frames. However, we need those frames to overlap to don't lose information on the edges of each frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0295a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def framing_audio(audio, FFT_size=2048, hop_size=10, sample_rate=22050):\n",
    "  \n",
    "    audio = np.pad(audio, int(FFT_size / 2), mode='reflect') # padding the signal to ensure that all samples in frame have equal lenght \n",
    "    frame_len = np.round(sample_rate * hop_size / 1000).astype(int) # convert frame lenght from seconds to samples\n",
    "    frame_num = int((len(audio) - FFT_size) / frame_len) + 1 # convert frame number from seconds to samples\n",
    "    frames = np.zeros((frame_num,FFT_size)) # define structure of frames\n",
    "    \n",
    "    for n in range(frame_num):\n",
    "        frames[n] = audio[n*frame_len:n*frame_len+FFT_size]\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def fft_spectrum(audio_framed, hop_size=544 , FFT_size=2048):    \n",
    "\n",
    "    ''' create window to limit audio frammed\n",
    "        hann window is used because this method is useful\n",
    "        to smoothing discontinuities in the edges of each frame'''\n",
    "    \n",
    "    window = get_window(\"hann\", FFT_size, fftbins=True) \n",
    "\n",
    "    audio_windowed = audio_framed * window # aply the smooth operation in audio framed\n",
    "\n",
    "    audio_windowedT = np.transpose(audio_windowed) # aply the tranpose to performed the fft\n",
    "    audio_fft = np.empty((int(1 + FFT_size // 2), audio_windowedT.shape[1]), dtype=np.complex64, order='F') # define structure of fft\n",
    "    for n in range(audio_fft.shape[1]):\n",
    "        audio_fft[:, n] = fft.fft(audio_windowedT[:, n], axis=0)[:audio_fft.shape[0]]\n",
    "\n",
    "    audio_fft = np.transpose(audio_fft) # invert the shape of resultant fft to the same of audio_windowed\n",
    "\n",
    "    audio_power = np.square(np.abs(audio_fft)) # get the power of signal\n",
    "    \n",
    "    return audio_power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cbd5a6",
   "metadata": {},
   "source": [
    "## MFCCs extraction\n",
    "\n",
    "To calculate the mfccs of each syllable we will follow the next steps:\n",
    "\n",
    "    - Extract the maximum and minimum frequencies in the signal based on spectral band width;\n",
    "    \n",
    "    - Create filter banks based on these frequencies;\n",
    "    \n",
    "    - Multiply filters with signal in power;\n",
    "    \n",
    "    - Calculate the mfccs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99cc2f",
   "metadata": {},
   "source": [
    "#### Get the maximum and minimum frequencies of spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acc22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_minimum_hz(audio, show_graph = False):\n",
    "        \n",
    "    S, _ = magphase(stft(y=audio)) # perform stft in the signal\n",
    "    spec_bw = feature.spectral_bandwidth(S=S) # obtain the spectral band to get max and min frequencies\n",
    "    \n",
    "    if show_graph:\n",
    "        plt.figure()\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.semilogy(spec_bw.T, label='Spectral bandwidth')\n",
    "        plt.ylabel('Hz')\n",
    "        plt.xticks([])\n",
    "        plt.xlim([0, spec_bw.shape[-1]])\n",
    "        plt.legend()\n",
    "        plt.subplot(2, 1, 2)\n",
    "        display.specshow(amplitude_to_db(S, ref=np.max),\n",
    "                                 y_axis='log', x_axis='time')\n",
    "        plt.title('log Power spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return np.round(np.min(spec_bw), decimals=4), np.round(np.max(spec_bw), decimals=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500134a5",
   "metadata": {},
   "source": [
    "#### Construction of filter bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert hz to mel scale\n",
    "def freq_to_mel(freq):\n",
    "    return np.round(1125 * ln(1.0 + freq / 700), decimals=4)\n",
    "\n",
    "# convert mel scale to hz\n",
    "def mel_to_freq(mels):\n",
    "    return 700 * (exp(mels / 1125) - 1)\n",
    "\n",
    "\n",
    "def get_filter_points(fmin, fmax, mel_filter_num, FFT_size, sample_rate=22050):\n",
    "    freqs = []\n",
    "    filter_bank = []\n",
    "    \n",
    "    fmin_mel = freq_to_mel(fmin) # convert initial frequency limit from Hz to mel\n",
    "    fmax_mel = freq_to_mel(fmax) # convert final frequency limit from Hz to mel\n",
    "    \n",
    "    print(f'Minimum mel frequency: {fmin_mel}')\n",
    "    print(f'Maximum mel frequency: {fmax_mel}')\n",
    "     \n",
    "    mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num + 2) # construct linear array between the frequencies limits\n",
    "    \n",
    "    [freqs.append(mel_to_freq(mel)) for mel in mels]  # convert the linear array from mel scale to Hz    \n",
    "    [filter_bank.append(np.floor((FFT_size + 1) / sample_rate * freq).astype(int)) for freq in freqs]  # normalize values to FFT size\n",
    "\n",
    "    filters = np.zeros((len(filter_bank)-2,int(FFT_size/2+1))) # define the structure of filters\n",
    "\n",
    "    for n in range(len(filter_bank)-2):\n",
    "        \n",
    "        #define the lower limit of the filter\n",
    "        filters[n, filter_bank[n] : filter_bank[n + 1]] = np.linspace(0, 1, filter_bank[n + 1] - filter_bank[n]) # this function will produce a come up from 0 to 1 in the begining point until the after\n",
    "        \n",
    "        #define the upper limit of the filter\n",
    "        filters[n, filter_bank[n + 1] : filter_bank[n + 2]] = np.linspace(1, 0, filter_bank[n + 2] - filter_bank[n + 1]) # this function will produce a come down from 1 to 0 in the final point until the afte\n",
    "        \n",
    "    \n",
    "    # saw in the librosa library\n",
    "    freqs = np.array(freqs) # convert list to array\n",
    "    \n",
    "    enorm = 2.0 / (freqs[2:mel_filter_num+2] - freqs[:mel_filter_num]) # divide the triangular MEL filter by the width of the MEL band (area normalization)\n",
    "    filters *= enorm[:, np.newaxis]\n",
    "    \n",
    "    return filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326170b8",
   "metadata": {},
   "source": [
    "#### Calculation of mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfccs(filters, audio_power, syllable):\n",
    "        \n",
    "    audio_filtered_mfccs = np.dot(filters, np.transpose(audio_power)) # aply the filters into the spectrum calculated above\n",
    "    audio_log = 10.0 * np.log10(audio_filtered_mfccs) # convert to log scale\n",
    "    audio_log.shape\n",
    "\n",
    "\n",
    "    mel_coeficients = 40\n",
    "\n",
    "    coefficients = np.transpose(fft.dct(audio_log, type=3)) # will be calculated the type 3 dct to distinguish from high to low frequencies\n",
    "    mfccs_values = np.dot(coefficients, audio_log) # aply the number of coeficients in the spectrum\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.imshow(mfccs_values, aspect='auto', origin='lower',cmap='coolwarm')\n",
    "    plt.savefig(syllable)\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f3188",
   "metadata": {},
   "source": [
    "## After we split the word in phonemes let's create a new phonetic dataset !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd1f121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_written_word(word):\n",
    "    dic = pyphen.Pyphen(lang='pt')\n",
    "    return dic.inserted(word).split('-')\n",
    "\n",
    "def create_phonetic_dataset():\n",
    "    hop_size = 10 # ms\n",
    "    FFT_size = 2048 # size of frame\n",
    "    pos = 0\n",
    "    name_example_0 = 0\n",
    "    name_example_1 = 0\n",
    "\n",
    "    principal_folder = 'Dataset'\n",
    "    sub_folder = 'Phonemes'\n",
    "    if not exists(join(principal_folder,sub_folder)): # check if the folder already exists else create \n",
    "        os.mkdir(join(principal_folder, sub_folder))\n",
    "\n",
    "    for i in tqdm(range(len(splited_phonemes))):\n",
    "        wave = filtered_audios[i][1] # get audio wave\n",
    "        name = filtered_audios[i][0] # get the word and the person\n",
    "        name = name.split('\\\\')[2] # get the word said\n",
    "        phonetics = split_written_word(name) # split the word in syllables\n",
    "\n",
    "        while pos != len(splited_phonemes[i]) - 2:\n",
    "            audio = wave[splited_phonemes[i][pos]:splited_phonemes[i][pos+1]] # get the wave per syllable\n",
    "            if pos == 0:\n",
    "                number = name_example_0\n",
    "                phonetic = phonetics[0] # get the corresponding syllable from wave\n",
    "                name_example_0 += 1\n",
    "            else:\n",
    "                number = name_example_1\n",
    "                phonetic = phonetics[1] # get the corresponding syllable from wave\n",
    "                name_example_1 += 1\n",
    "\n",
    "            if not exists(join(principal_folder,sub_folder, phonetic)): # check if the folder already exists else create \n",
    "                os.mkdir(join(principal_folder, sub_folder,phonetic))  \n",
    "\n",
    "            audio_normalized = normalize_audio(audio) # normalize audio\n",
    "            audio_framed = framing_audio(audio_normalized, FFT_size=FFT_size, hop_size=hop_size, sample_rate=22050) # framming the audio\n",
    "            audio_power = fft_spectrum(audio_framed, hop_size=hop_size, FFT_size=FFT_size) # perform fft and convert to power\n",
    "\n",
    "            freq_min, freq_max = maximum_minimum_hz(audio, show_graph = False) # get the minimun and the maximum frequencies of the audio\n",
    "            filters_to_mfcc =  get_filter_points(freq_min, freq_max, mel_filter_num=10, FFT_size=FFT_size, sample_rate=22050) # construct filter bank\n",
    "\n",
    "            mfccs(filters_to_mfcc, audio_power, join(principal_folder, sub_folder,phonetic,str(number)))  # calculate the mfccs      \n",
    "            pos += 1\n",
    "        pos = 0\n",
    "        \n",
    "create_phonetic_dataset()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418320a",
   "metadata": {},
   "source": [
    "## Train SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset):\n",
    "    \n",
    "    X = [] # list initialization\n",
    "    y = [] # list initialization\n",
    "\n",
    "    for samples, label in dataset:\n",
    "        X.append(samples)\n",
    "        y.append(label)\n",
    "    \n",
    "    X= np.array(X).reshape(len(dataset),-1) # reshape the samples\n",
    "\n",
    "    X = X/255.0 # normalize samples between 0 and 1\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, stratify=y) # split data to train and test\n",
    "\n",
    "    print(f'The model will be trained with {X_train.shape[0]} and tested with {X_test.shape[0]} samples.','\\n')\n",
    "    \n",
    "    start = time.time()   \n",
    "    svc = SVC(kernel='linear',gamma='auto') # define the model\n",
    "    svc.fit(X_train, y_train) # train the model\n",
    "    end = time.time()\n",
    "     \n",
    "    print(f'The model took {(end-start)/60} minutes to train.', '\\n')    \n",
    "          \n",
    "    filename = 'final1_model.sav'\n",
    "    pickle.dump(svc, open(filename, 'wb')) # save model\n",
    "    \n",
    "    evalute_model(X_test, y_test, filename)\n",
    "    \n",
    "def evalute_model(X_test, y_test, filename):\n",
    "    \n",
    "    model = pickle.load(open(filename, 'rb')) # load model\n",
    "    predictions = model.predict(X_test) # testing test set\n",
    "    \n",
    "    report_dict = classification_report(y_test, predictions, output_dict=True) # get the classification report\n",
    "    report = pd.DataFrame(report_dict)\n",
    "    report.to_csv('reports/report.csv', index = False) # save the classification report\n",
    "    \n",
    "    print(f'Model has an accuracy of {accuracy_score(y_test,predictions)* 100} % on test set')\n",
    "    \n",
    "    cm = confusion_matrix(y_test, predictions, labels=model.classes_) # create confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_) # convert confusion matrix to graph\n",
    "    disp.plot()\n",
    "    plt.savefig('Confusion matrix')\n",
    "    \n",
    "def pre_process_data(folder):\n",
    "    training_data = []   \n",
    "    for file in dataset_folder:\n",
    "        images = glob.glob(f'{file}/*') # get all examples inside each phoneme folder \n",
    "        for image in images:\n",
    "            example = cv2.imread(image, cv2.IMREAD_GRAYSCALE) # read img and convert to gray scale\n",
    "            example = cv2.resize(example,[360,480]) # resize img\n",
    "            training_data.append([example,file.split('\\\\')[1]])\n",
    "    \n",
    "    print('Dataset is pre processed.')      \n",
    "    print(f'Dataset contains a total of {len(training_data)} examples.', '\\n')\n",
    "    train_model(training_data)\n",
    "    \n",
    "dataset_folder = glob.glob('Dataset/Phonemes/*')  # get all folders inside Dataset folder\n",
    "pre_process_data(dataset_folder)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c523ad",
   "metadata": {},
   "source": [
    "### Calculate the fundamental frequency of each person\n",
    "\n",
    "- The pitch is the pulse frequency that a person makes to create sound like words or simply noise. So if we know this frequency we will be capable to identify the person who is speaking;\n",
    "\n",
    "\n",
    "- Considering all people (children, men, and women) the typical range of  __f0 is between 80 Hz and 500 Hz;__\n",
    "\n",
    "\n",
    "- In the cells below will be shown how it's possible to identify the pitch (f0) of each person. \n",
    "\n",
    "\n",
    "- Pitch will be calculated with three methods:\n",
    "    1. __PYin__ method;\n",
    "    2. __Cepstrum__ method.\n",
    "    3. __Parselmouth library__ (praat method).\n",
    "    \n",
    "<img src=\"resources/voice-authentication-01.png\" width=\"460\" height=\"460\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9302a",
   "metadata": {},
   "source": [
    "### PYin method\n",
    "This method derives from *__yin__* method, and both are available in librosa library. https://librosa.org/doc/main/generated/librosa.yin.html?highlight=yin#librosa.yin\n",
    "https://librosa.org/doc/main/generated/librosa.pyin.html?highlight=pyin#librosa.pyin\n",
    "\n",
    "Both methods are based on autocorrelation, however the way that those methods define the F0 is different. While yin use a single threshold during all signal, pyin use many thresholds based on probability and that's why it's able to predict if there or is not voice in signal.\n",
    "\n",
    "<img src=\"resources/f0_estimation_ex.png\" width=\"460\" height=\"460\">\n",
    "\n",
    "In the image above it is possible to compare the two methods and verify if the yin predicts F0 where the signal has no voice, which is wrong and makes it impossible to obtain the most correct value of the person's F0. For this reason, we will __only use pyin.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeef9ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def p_yin(y):\n",
    "    \n",
    "    frame_length = 512 # define frame lenght\n",
    "    \n",
    "    f0, voiced_flag, voiced_probs = pyin(y=y, fmin=80,fmax=500,frame_length=frame_length) # extract pith using pyin\n",
    "    f0 = np.mean(f0[np.where(voiced_flag == True)])\n",
    "    \n",
    "    y_harmonic, y_percussive = effects.hpss(y) # extract harmonics and percurssives\n",
    "    y_harmonic = np.round(np.mean(y_harmonic), decimals=5)\n",
    "    y_percussive = np.round(np.mean(y_percussive), decimals=6)\n",
    "    \n",
    "    return f0, y_harmonic, y_percussive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4dbee",
   "metadata": {},
   "source": [
    "### Cepstrum method\n",
    "This method is the result of the inverse FFT, which means that instead of using frequency and spectrum will be used quefrency and cepstrum. The main advantage of this method is that the peak of fundamental frequency will be more highlighted when compared to the spectrum. The resultant graph shows two highlighted peaks, where the first marks the frequency and the second is the F0 of a person.\n",
    "\n",
    "<img src=\"resources/cpe_vs_spe.png\" width=\"460\" height=\"460\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cepstrum(audio, sample_freq):\n",
    "    \n",
    "    windowed_signal = np.hamming(audio.size) * signal # create windowed signal\n",
    "    \n",
    "    freq_vector = np.fft.fftfreq(frame_size, d=1/sample_freq) # create frequency vector to fft\n",
    "    X = np.fft.fft(windowed_signal) # aply fft\n",
    "    log_X = np.log(np.abs(X)) # convert fft to power spectrum\n",
    "    \n",
    "    cepstrum = np.fft.fft(log_X) # convert spectrum to cepstrum\n",
    "    df = freq_vector[1] - freq_vector[0] # create a vector size\n",
    "    quefrency_vector = np.fft.fftfreq(log_X.size, df) # get the quefrency values\n",
    "    \n",
    "    return quefrency_vector, cepstrum\n",
    "\n",
    "def extract_ceps_f0(signal, sample_freq=22050, fmin=80, fmax=500):\n",
    "    \"\"\"Returns f0 based on cepstral processing.\"\"\"\n",
    "    quefrency_vector, cepstrum = extract_cepstrum(signal, sample_freq) # extract cepstrum\n",
    "    \n",
    "    valid = (quefrency_vector > 1/fmax) & (quefrency_vector <= 1/fmin) # get the peaks only in F0 range\n",
    "    max_quefrency_index = np.argmax(np.abs(cepstrum)[valid]) # get the index of max peak\n",
    "    f0 = 1/quefrency_vector[valid][max_quefrency_index] # ensure that the peak is in f0 range \n",
    "    \n",
    "    return f0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9a34d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94fa94",
   "metadata": {},
   "source": [
    "### Parselmouth library\n",
    "This code is taken from https://github.com/drfeinberg/PraatScripts.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measurePitch(voiceID, f0min, f0max, unit):\n",
    "    sound = parselmouth.Sound(voiceID) # read the sound\n",
    "    pitch = call(sound, \"To Pitch\", 0.0, f0min, f0max) #create a praat pitch object\n",
    "    meanF0 = call(pitch, \"Get mean\", 0, 0, unit) # get mean pitch\n",
    "    stdevF0 = call(pitch, \"Get standard deviation\", 0 ,0, unit) # get standard deviation\n",
    "    harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "    hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "    \n",
    "    return meanF0, stdevF0, hnr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f614a75",
   "metadata": {},
   "source": [
    "### Extract fundamental frequency using the three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch():\n",
    "    \n",
    "    # lists initialization\n",
    "    \n",
    "    file_list = []\n",
    "    mean_F0_praat = []\n",
    "    sd_F0 = []\n",
    "    hnr_list = []\n",
    "    \n",
    "    mean_F0_pyin = []\n",
    "    mean_harms = []\n",
    "    mean_percs = []\n",
    "    \n",
    "    mean_F0_ceps = []\n",
    "\n",
    "    # Go through all the wave files in the folder and measure pitch\n",
    "    for i in tqdm(range(len(filtered_audios))):\n",
    "        wave = filtered_audios[i][1] # get the wave\n",
    "        name = filtered_audios[i][0].split('\\\\')[1] # get the person name\n",
    "        file_list.append(name) # make an ID list\n",
    "        \n",
    "        y = wave[audio_flag[i][0]:audio_flag[i][1]] # get only the marker words in audio\n",
    "\n",
    "        # Parselmouth extraction\n",
    "        sound = parselmouth.Sound(y)\n",
    "        meanF0, stdevF0, hnr = measurePitch(sound, 80, 500, \"Hertz\")         \n",
    "        mean_F0_praat.append(meanF0) \n",
    "        sd_F0.append(stdevF0)\n",
    "        hnr_list.append(hnr)\n",
    "\n",
    "        # PYin extraction\n",
    "        f0, harmonic, percurssive = p_yin(y)\n",
    "        mean_F0_pyin.append(f0)\n",
    "        mean_harms.append(harmonic)\n",
    "        mean_percs.append(percurssive)\n",
    "        \n",
    "        # Cepstrum extraction\n",
    "        f0 = extract_ceps_f0(y)\n",
    "        mean_F0_ceps.append(f0)\n",
    "        \n",
    "        \n",
    "    praat = pd.DataFrame(np.column_stack([file_list, mean_F0_list, sd_F0_list, hnr_list]), \n",
    "                               columns=['voiceID', 'meanF0 _Hz', 'stdevF0 _Hz', 'HNR'])  #add these lists to pandas in the right order\n",
    "    # Write out the updated dataframe\n",
    "    praat.to_csv(\"pitch_range_praat.csv\", index=False)    \n",
    "\n",
    "    pyin = pd.DataFrame(np.column_stack([file_list, mean_F0_pyin, mean_harms, mean_percs]), \n",
    "                                   columns=['voiceID', 'meanF0_Hz', 'Harmonics', 'Percurssives'])  #add these lists to pandas in the right order\n",
    "    # Write out the updated dataframe\n",
    "    pyin.to_csv(\"pitch_range_praat.csv\", index=False)    \n",
    "\n",
    "    cepst = pd.DataFrame(np.column_stack([file_list, mean_F0_ceps]), \n",
    "                                   columns=['voiceID', 'meanF0_Hz'])  #add these lists to pandas in the right order\n",
    "    # Write out the updated dataframe\n",
    "    cepst.to_csv(\"pitch_range_praat.csv\", index=False)    \n",
    "    \n",
    "get_pitch()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "i = 1\n",
    "wave = filtered_audios[i][1] # get audio wave\n",
    "print(splited_phonemes[i])\n",
    "y = wave[44000:49000]\n",
    "ipd.Audio(y,rate=22050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 542.855364,
   "position": {
    "height": "564.838px",
    "left": "-225.025px",
    "right": "20px",
    "top": "-322.038px",
    "width": "296.95px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
