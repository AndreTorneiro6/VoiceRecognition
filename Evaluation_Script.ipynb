{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef790a0a",
   "metadata": {},
   "source": [
    "# This script will evaluate the dataset created and identify the speaker\n",
    "Note:\n",
    "- If you doesn't have the dataset created please run word_identification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeec348",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "Run the next cell only if you don't have the packages already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scipy matplotlib sounddevice wave playsound tqdm librosa pyphen opencv-python praat-parselmouth pyttsx3 pydub ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572b697",
   "metadata": {},
   "source": [
    "## <center> <span style='color :blue' >Sound Analysis</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac04217",
   "metadata": {},
   "source": [
    "## Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c157efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import all the required libraries in code below\n",
    "from librosa import load, get_duration, display, feature, magphase, frames_to_time, onset, autocorrelate, effects, stft, amplitude_to_db, times_like, pyin\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score, classification_report\n",
    "from scipy.signal import lfilter, lfilter_zi, filtfilt, butter, freqz, spectrogram, get_window, find_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyphen, math, time, glob, os, cv2, pickle, parselmouth\n",
    "from librosa.util import fix_length\n",
    "from parselmouth.praat import call\n",
    "# from pydub import AudioSegments\n",
    "from os.path import join, exists\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from sklearn.svm import SVC\n",
    "import scipy.fftpack as fft\n",
    "from math import log as ln\n",
    "from tqdm import tqdm\n",
    "from math import exp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyttsx3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90acf4d",
   "metadata": {},
   "source": [
    "# Convert audio extension from m4a to wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4a_file = '20211210_151013.m4a'\n",
    "wav_filename = r\"F:\\20211210_151013.wav\"\n",
    "\n",
    "track = AudioSegment.from_file(m4a_file,  format= 'm4a')\n",
    "file_handle = track.export(wav_filename, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a536fc",
   "metadata": {},
   "source": [
    "### Read and View the graph of  audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfcc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio_signal(audio_file):    \n",
    "    audio, fs = load(audio_file) # read audio file with fs = 22050 in mono mode\n",
    "\n",
    "    padded_audio = fix_length(audio, size=4*fs) # set the audio with 4 seconds\n",
    "    \n",
    "    duration_original = get_duration(y=audio, sr=fs) # get the duration of the audio in seconds\n",
    "    duration_padded = get_duration(y=padded_audio, sr=fs) # get the duration of the audio padded in seconds\n",
    "    \n",
    "    duration_time_original = np.linspace(0, duration_original, audio.shape[0]) # create time vector to x-scale in graph\n",
    "    duration_time_padded = np.linspace(0, duration_padded, padded_audio.shape[0]) # create time vector to x-scale in graph\n",
    "\n",
    "    n_points_original = int(fs * duration_original) # signal size in points original\n",
    "    n_points_padded = int(fs * duration_padded) # signal size in points padded\n",
    "    \n",
    "    return fs, audio, padded_audio, duration_time_original, duration_time_padded, n_points_original, n_points_padded\n",
    "        \n",
    "def show_orginal_audio(filename): # show original wave of each word\n",
    "    fs, audio_orig, audio_pad,time_orig, time_paded, _, _ = read_audio_signal(filename) # read audio example\n",
    "    \n",
    "    # define ghapics parameters \n",
    "    plt.rcParams['figure.figsize'] = [15, 7]\n",
    "    plt.tight_layout(pad=3.0) \n",
    "    fig, ax = plt.subplots(ncols=2, sharex=False)\n",
    "\n",
    "    fig.suptitle(filename.split('/')[1] + ' saying ' +  filename.split('/')[-2], fontsize = 16, fontweight = 'bold')\n",
    "    \n",
    "    ax[0].plot(time_orig ,audio_orig) # show graph with original audio signal\n",
    "    ax[0].set(title='Original audio')\n",
    " \n",
    "    ax[1].set(title ='Padded audio')\n",
    "    ax[1].plot(time_paded ,audio_pad) # show graph with padded audio signal\n",
    "    \n",
    "    for ax in ax.flat:\n",
    "        ax.set(xlabel='Time [s]', ylabel='Amplitude')\n",
    "    plt.show()   \n",
    "    \n",
    "\n",
    "show_orginal_audio('Dataset/Andre/casa/0.wav') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c21e25",
   "metadata": {},
   "source": [
    "## Filter the original signal between typical voice frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca08bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_original_voice(filename):\n",
    "\n",
    "    filtered_audio = [] # initialize list\n",
    "    \n",
    "    fs, _, audio_readed, _, time_paded, _, _ = read_audio_signal(filename) # read audio example\n",
    "          \n",
    "    nyq = 0.5 * fs # nyquist frequency\n",
    "\n",
    "    # normalize cutoff frequency\n",
    "    low = 60 / nyq \n",
    "    high = 7000 / nyq \n",
    "\n",
    "    b, a = butter(6, [low, high], btype='band') # define 5th order band-pass butterworth filter \n",
    "\n",
    "    w, h = freqz(b, a) # compute frequency response\n",
    "    \n",
    "    # show frequency response\n",
    "    plt.plot((fs * 0.5 / np.pi) * w, 20 * np.log10(abs(h)), 'b')\n",
    "    plt.ylabel('Amplitude [dB]', color='b')\n",
    "    plt.xlabel('Frequency [rad/sample]')   \n",
    "    plt.title('Frequency Response',  fontsize = 16, fontweight = 'bold')  \n",
    "    plt.grid(True)\n",
    "    plt.xticks(range(0, 12000, 1000))\n",
    "    plt.ylim(-500, 10)\n",
    "\n",
    "    # Clearer view of frequency response at low cutoff frequency\n",
    "    ax1 =  plt.axes([0.2, 0.3, 0.2, 0.2])\n",
    "    ax1.plot((fs * 0.5 / np.pi) * w, 20 * np.log10(abs(h)), 'r')\n",
    "    plt.title('Zoom in low cutoff')   \n",
    "    plt.xlim(0,100)\n",
    "    plt.ylim(-200, 10)\n",
    "\n",
    "    # Clearer view of frequency response at high cutoff frequency\n",
    "    ax2 =  plt.axes([0.6, 0.3, 0.2, 0.2])\n",
    "    ax2.plot((fs * 0.5 / np.pi) * w, 20 * np.log10(abs(h)), 'r')\n",
    "    plt.title('Zoom in high cutoff')   \n",
    "    plt.xlim(5000,8000)\n",
    "    plt.ylim(-10, 10)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    zi = lfilter_zi(b, a) # set initial state of the filter\n",
    "    z, _ = lfilter(b, a, audio_readed, zi=zi*audio_readed[0]) # filter signal from left to right\n",
    "    z2, _ = lfilter(b, a, z, zi=zi*z[0]) # filter signal from right to left\n",
    "    audio_filtered = filtfilt(b, a, audio_readed) # filter signal forward and backward\n",
    "\n",
    "    filtered_audio.append(filename) # save name and filtered audio in list\n",
    "    filtered_audio.append(audio_filtered) # save audio filtered in list\n",
    "    filtered_audio.append(fs) # save fs in list\n",
    "    \n",
    "\n",
    "    # define graphics parameters \n",
    "    plt.rcParams['figure.figsize'] = [15, 7]\n",
    "    plt.tight_layout(pad=3.0)\n",
    "\n",
    "    # show graph with original audio signal\n",
    "    plt.plot(time_paded, audio_readed, 'r', linewidth = 3)\n",
    "\n",
    "    # show graph with filtered audio signal\n",
    "    plt.plot(time_paded,audio_filtered, 'g', alpha = 0.8)\n",
    "\n",
    "    plt.legend(('original signal','Filtered signal'), loc = 3, prop={'size': 12})  \n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title(filename.split('/')[1] + ' saying ' +  filename.split('/')[-2], fontsize = 16, fontweight = 'bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    print('All audios are filtered')\n",
    "    return filtered_audio\n",
    "\n",
    "\n",
    "# call function to visualize frequency response and graphic of each filtered audio\n",
    "filtered_audio = filter_original_voice('Dataset/Andre/casa/0.wav') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16163b53",
   "metadata": {},
   "source": [
    "### View spectrogram to see where are the power and silence in filtered signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "def see_spectrogram_scipy():\n",
    "    \n",
    "    name = filtered_audio[0] # get the word and the person\n",
    "    wave = filtered_audio[1] # get audio wave\n",
    "    fs = filtered_audio[2] # get sample rate\n",
    "        \n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [15, 15]\n",
    "    plt.tight_layout(pad=3.0)\n",
    "\n",
    "    # perfom spectogram analysis\n",
    "    f, t, S1 = spectrogram(wave, fs ,window='flattop', nperseg=fs//10, noverlap=fs//20, scaling='density', mode='magnitude')\n",
    "\n",
    "    # show graph with spectogram of each audio\n",
    "    plt.pcolormesh(t, f[:800], S1[:800][:])\n",
    "    plt.xlabel('time(s)')\n",
    "    plt.ylabel('frequency(Hz)')\n",
    "    plt.title(name.split('/')[1]+ ' saying ' + name.split('/')[-2], fontsize = 16, fontweight = 'bold')\n",
    "\n",
    "    plt.show()\n",
    "     \n",
    "        \n",
    "def see_spectrogram_librosa (y_scale = 'log'):\n",
    "            \n",
    "    name = filtered_audio[0] # get the word and the person\n",
    "    wave = filtered_audio[1] # get audio wave\n",
    "    fs = filtered_audio[2] # get sample rate\n",
    "\n",
    "\n",
    "    X = stft(wave) # aply stft to filtered signal\n",
    "    Xdb = amplitude_to_db(abs(X)) # convert signal only to positive values and convert amplitude to db\n",
    "\n",
    "    # define ghap parameters \n",
    "    plt.rcParams['figure.figsize'] = [15, 7]\n",
    "    plt.tight_layout(pad=3.0)\n",
    "\n",
    "    # show graph with spectogram of each audio\n",
    "    display.specshow(Xdb[:800], sr=fs, x_axis='time', y_axis= y_scale)\n",
    "    plt.title(name.split('/')[1]+ ' saying ' + name.split('/')[-2], fontsize = 16, fontweight = 'bold')\n",
    "    plt.colorbar()\n",
    "    \n",
    "# see_spectrogram_librosa()\n",
    "see_spectrogram_scipy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7a78f",
   "metadata": {},
   "source": [
    "### Split the audio in silence and non-silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_consecutive(lst, distance = 3):\n",
    "    \n",
    "    # for loop to iterate the list   \n",
    "    for i,j in enumerate(lst,lst[0]): \n",
    "        \n",
    "        # get the non consecutive values which represent words in the audio\n",
    "        if i!=j and j - i > distance: \n",
    "            return [i,j], j\n",
    "                \n",
    "\n",
    "def calculate_rms(show_graph = False):  \n",
    "    \n",
    "    get_exp_value = False # bool variable to indicate if exponential value is already known\n",
    "    audio_flag= []\n",
    "    frame_length = 2048\n",
    "    hop_length = 1024\n",
    "    \n",
    "    name = filtered_audio[0] # get the word and the person\n",
    "    wave = filtered_audio[1] # get audio wave\n",
    "    fs = filtered_audio[2] # get sample rate\n",
    "        \n",
    "    # determine average power in each frame\n",
    "    rms = feature.rms(y=abs(wave), frame_length=frame_length, hop_length=hop_length, center=True)[0]\n",
    "\n",
    "    rms_rounded = np.round(rms, decimals=2) # round rms values in 2 decimals\n",
    "\n",
    "    if not get_exp_value:\n",
    "        difference_value = (len(wave)/len(rms_rounded))\n",
    "        x = math.floor(math.log10(difference_value))  # x = 3\n",
    "        exp_value = 1 * 10**x                       \n",
    "        get_exp_value = True\n",
    "\n",
    "    # list to return values where audio is\n",
    "    silence_time = []\n",
    "    for rms_index in range(len(rms_rounded)):\n",
    "            if rms_rounded[rms_index] == 0:\n",
    "                silence_time.append(rms_index)\n",
    "\n",
    "    marker_phrase, index = find_non_consecutive(silence_time) # get the limits of phrase in audio segment    \n",
    "    marker_words_limit = silence_time.index(index) # find the index of the limit in marker phrase\n",
    "\n",
    "    del silence_time[:marker_words_limit] # delete all values in list until the limit to research\n",
    "    revaluate_word_position, _ = find_non_consecutive(silence_time) # get the limits of word in audio segment\n",
    "\n",
    "    phrase_begin = np.multiply(np.min(marker_phrase), exp_value) # get the value where phrase begin\n",
    "    phrase_end = np.multiply(np.max(marker_phrase), exp_value) # get the value where phrase end\n",
    "\n",
    "    word_begin = np.multiply(np.min(revaluate_word_position), exp_value) # get the value where word begin     \n",
    "    word_end = np.multiply(np.max(revaluate_word_position),exp_value) # get the value where word end\n",
    "\n",
    "    audio_flag.append(phrase_begin)    \n",
    "    audio_flag.append(phrase_end)   \n",
    "    audio_flag.append(word_begin)   \n",
    "    audio_flag.append(word_end)   \n",
    "\n",
    "    # get number of points in signal and the respective time\n",
    "    frames_f = range(len(rms_rounded))\n",
    "    time_scale = frames_to_time(frames_f, sr=fs, hop_length=hop_length)\n",
    "\n",
    "    # define ghap parameters \n",
    "    plt.rcParams['figure.figsize'] = [15, 7]\n",
    "    plt.tight_layout(pad=3.0)\n",
    "\n",
    "    # show graph with rms of each audio\n",
    "    plt.plot(time_scale, rms_rounded, 'g')\n",
    "    plt.title(name.split('/')[1]+ ' saying ' + name.split('/')[-2], fontsize = 16, fontweight = 'bold')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return audio_flag\n",
    "\n",
    "audio_flag = calculate_rms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4197e1",
   "metadata": {},
   "source": [
    "## Extract number of syllables in audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98772ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_syllables():\n",
    "     \n",
    "    splited_phonemes = []    \n",
    "        \n",
    "    name = filtered_audio[0] # get the word and the person\n",
    "    wave = filtered_audio[1] # get audio wave\n",
    "    fs = filtered_audio[2] # get sample rate\n",
    "    \n",
    "    y = wave[audio_flag[2]:audio_flag[3]] # get only the word\n",
    "\n",
    "    notes_changes = onset.onset_strength(y=y, sr=fs, hop_length=512) # extract onset transitions\n",
    "    ac = autocorrelate(notes_changes/notes_changes.max(), max_size= fs // 512) # aply autocorrelation to onset transitions\n",
    "    peaks, _ = find_peaks(ac, height=0.15) # find peak with a minimum height\n",
    "\n",
    "    if len(peaks):\n",
    "        syllable_limit =  audio_flag[2] + math.floor(peaks[-1]/2) * 1000 + 1000 # divid the audio based on the peak from autocorrelation\n",
    "    else:\n",
    "        syllable_limit =  audio_flag[2] + math.floor((len(y)/1000)/2) * 1000 + 1000 \n",
    "\n",
    "    splited_phonemes.append(audio_flag[2]) # save the time where first syllable beggin\n",
    "    splited_phonemes.append(syllable_limit) # save the time where first syllable end or second syllable beggin\n",
    "    splited_phonemes.append(audio_flag[3]) # save the time where second syllable end\n",
    "    splited_phonemes.append(name) \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(ac)\n",
    "    ax.set(title='Auto-correlation', xlabel='Lag (frames)')\n",
    "    plt.plot(peaks, ac[peaks], \"x\")\n",
    "    plt.show()\n",
    "        \n",
    "    print('All words are splited in phonemes')\n",
    "    \n",
    "    return splited_phonemes\n",
    "\n",
    "splited_phonemes = split_audio_syllables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a56aaf",
   "metadata": {},
   "source": [
    "### Let's listen the phonemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440211d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = filtered_audio[1] # get audio wave\n",
    "\n",
    "'''change the line below to listen the two phonemes like that:\n",
    "splited_phonemes[0]:splited_phonemes[1] or splited_phonemes[1]:splited_phonemes[2]'''\n",
    "\n",
    "y = wave[splited_phonemes[1]:splited_phonemes[2]] # listen only the individual phoneme\n",
    "ipd.Audio(y,rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67da35",
   "metadata": {},
   "source": [
    "#### Normalize audio values after pre-processement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_audio(audio, name):\n",
    "    positive_audio = abs(audio) # convert audio to absolute values\n",
    "    audio = positive_audio / np.max(positive_audio) # divide audio values by max value in audio to normalize\n",
    "    \n",
    "    # define graph parameters and plot graph\n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.plot(np.linspace(0, len(positive_audio) / 22050, num=len(positive_audio)), audio)\n",
    "    plt.title(f'Audio Normalization of {name}', fontsize = 16, fontweight = 'bold')\n",
    "    plt.grid(True)\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42ab61",
   "metadata": {},
   "source": [
    "#### Aply framimg in audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4948487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def framing_audio(audio, FFT_size=1024, hop_size=10, sample_rate=22050):\n",
    "  \n",
    "    audio = np.pad(audio, int(FFT_size / 2), mode='reflect') # padding the signal to ensure that all samples in frame have equal lenght \n",
    "    frame_len = np.round(sample_rate * hop_size / 1000).astype(int) # convert frame lenght from seconds to samples\n",
    "    frame_num = int((len(audio) - FFT_size) / frame_len) + 1 # convert frame number from seconds to samples\n",
    "    frames = np.zeros((frame_num,FFT_size)) # define structure of frames\n",
    "    \n",
    "    for n in range(frame_num):\n",
    "        frames[n] = audio[n*frame_len:n*frame_len+FFT_size]\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def fft_spectrum(audio_framed, hop_size=10 , FFT_size=1024):    \n",
    "\n",
    "    ''' create window to limit audio frammed\n",
    "        hann window is used because this method is useful\n",
    "        to smoothing discontinuities in the edges of each frame'''\n",
    "    \n",
    "    window = get_window(\"hann\", FFT_size, fftbins=True) \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.plot(window)\n",
    "    plt.grid(True)\n",
    "    plt.title('Window applied to frammimg')\n",
    "    plt.show()\n",
    "\n",
    "    audio_windowed = audio_framed * window # aply the smooth operation in audio framed\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.plot(audio_framed[0], label='Original Frame' )\n",
    "    plt.plot(audio_windowed[0], label='Frame After Windowing' )\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title('Example of framming one frame')\n",
    "    plt.show\n",
    "\n",
    "    audio_windowedT = np.transpose(audio_windowed) # aply the tranpose to performed the fft\n",
    "    audio_fft = np.empty((int(1 + FFT_size // 2), audio_windowedT.shape[1]), dtype=np.complex64, order='F') # define structure of fft\n",
    "    for n in range(audio_fft.shape[1]):\n",
    "        audio_fft[:, n] = fft.fft(audio_windowedT[:, n], axis=0)[:audio_fft.shape[0]]\n",
    "\n",
    "    audio_fft = np.transpose(audio_fft) # invert the shape of resultant fft to the same of audio_windowed\n",
    "\n",
    "    audio_power = np.square(np.abs(audio_fft)) # get the power of signal\n",
    "    \n",
    "    return audio_power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5366408",
   "metadata": {},
   "source": [
    "#### Get the maximum and minimum frequencies of spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba55e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_minimum_hz(audio):\n",
    "        \n",
    "    S, _ = magphase(stft(y=audio, n_fft=1024, hop_length=10)) # perform stft in the signal\n",
    "    spec_bw = feature.spectral_bandwidth(S=S) # obtain the spectral band to get max and min frequencies\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "    times = times_like(spec_bw)\n",
    "    \n",
    "    ax[0].semilogy(times, spec_bw[0], label='Spectral bandwidth')\n",
    "    ax[0].set(title='Band frequencies')\n",
    "    ax[0].set(ylabel='Hz', xticks=[], xlim=[times.min(), times.max()])\n",
    "    ax[0].legend()\n",
    "\n",
    "    display.specshow(amplitude_to_db(S, ref=np.max),\n",
    "                             y_axis='log', x_axis='time', ax=ax[1])\n",
    "    plt.plot(times, spec_bw[0], label='Spectral centroid', color='w')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Power Spectrogram')\n",
    "    \n",
    "    return np.round(np.min(spec_bw), decimals=4), np.round(np.max(spec_bw), decimals=4)\n",
    "\n",
    "audio = maximum_minimum_hz(wave[splited_phonemes[1]:splited_phonemes[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cff7b",
   "metadata": {},
   "source": [
    "#### Construction of filter bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f166374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert hz to mel scale\n",
    "def freq_to_mel(freq):\n",
    "    return np.round(1125 * ln(1.0 + freq / 700), decimals=4)\n",
    "\n",
    "# convert mel scale to hz\n",
    "def mel_to_freq(mels):\n",
    "    return 700 * (exp(mels / 1125) - 1)\n",
    "\n",
    "\n",
    "def get_filter_points(fmin, fmax, mel_filter_num, FFT_size, sample_rate=22050):\n",
    "    freqs = []\n",
    "    filter_bank = []\n",
    "    \n",
    "    fmin_mel = freq_to_mel(fmin) # convert initial frequency limit from Hz to mel\n",
    "    fmax_mel = freq_to_mel(fmax) # convert final frequency limit from Hz to mel\n",
    "    \n",
    "    print(f'Minimum mel frequency: {fmin_mel}')\n",
    "    print(f'Maximum mel frequency: {fmax_mel}')\n",
    "    \n",
    "    mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num + 2) # construct linear array between the frequencies limits\n",
    "    \n",
    "    [freqs.append(mel_to_freq(mel)) for mel in mels]  # convert the linear array from mel scale to Hz    \n",
    "    [filter_bank.append(np.floor((FFT_size + 1) / sample_rate * freq).astype(int)) for freq in freqs]  # normalize values to FFT size\n",
    "\n",
    "    filters = np.zeros((len(filter_bank)-2,int(FFT_size/2+1))) # define the structure of filters\n",
    "\n",
    "    for n in range(len(filter_bank)-2):\n",
    "        \n",
    "        #define the lower limit of the filter\n",
    "        filters[n, filter_bank[n] : filter_bank[n + 1]] = np.linspace(0, 1, filter_bank[n + 1] - filter_bank[n]) # this function will produce a come up from 0 to 1 in the begining point until the after\n",
    "        \n",
    "        #define the upper limit of the filter\n",
    "        filters[n, filter_bank[n + 1] : filter_bank[n + 2]] = np.linspace(1, 0, filter_bank[n + 2] - filter_bank[n + 1]) # this function will produce a come down from 1 to 0 in the final point until the afte\n",
    "        \n",
    "    plt.figure(figsize=(15,4))\n",
    "    for n in range(filters.shape[0]):\n",
    "        plt.plot(filters[n])\n",
    "        plt.xlim(filter_bank[0]-5,filter_bank[-1]+5)    \n",
    "        plt.title('Filters bank')\n",
    "    \n",
    "    # saw in the librosa library\n",
    "    freqs = np.array(freqs) # convert list to array\n",
    "    enorm = 2.0 / (freqs[2:mel_filter_num+2] - freqs[:mel_filter_num]) # divide the triangular MEL filter by the width of the MEL band (area normalization)\n",
    "    filters *= enorm[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    for n in range(filters.shape[0]):\n",
    "        plt.plot(filters[n])\n",
    "        plt.xlim(filter_bank[0]-5,filter_bank[-1]+5)\n",
    "        plt.title('Filters bank normalized')\n",
    "        \n",
    "        \n",
    "    return filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2bb90",
   "metadata": {},
   "source": [
    "#### Calculation of mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfccs(filters, audio_power, phoneme):\n",
    "        \n",
    "    audio_filtered_mfccs = np.dot(filters, np.transpose(audio_power)) # aply the filters into the spectrum calculated above\n",
    "    audio_log = 10.0 * np.log10(audio_filtered_mfccs) # convert to log scale\n",
    "    audio_log.shape\n",
    "\n",
    "\n",
    "    mel_coeficients = 20\n",
    "\n",
    "    coefficients = np.transpose(fft.dct(audio_log, type=3)) # will be calculated the type 3 dct to distinguish from high to low frequencies\n",
    "    mfccs_values = np.dot(coefficients, audio_log) # aply the number of coeficients in the spectrum\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.imshow(mfccs_values, aspect='auto', origin='lower',cmap='coolwarm')\n",
    "    plt.savefig(phoneme)\n",
    "    plt.title(f'Mfcc graph of phoneme {phoneme}')\n",
    "    \n",
    "    predict = evaluate_model(phoneme)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d2b0a",
   "metadata": {},
   "source": [
    "## Let's identify the phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c438db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def split_written_word(word):\n",
    "    # this function is to split word in syllables\n",
    "    dic = pyphen.Pyphen(lang='pt')\n",
    "    return dic.inserted(word).split('-')\n",
    "\n",
    "def split_phoneme_to_test():\n",
    "    hop_size = 10 # ms\n",
    "    FFT_size = 1024 # size of frame\n",
    "    pos = 0\n",
    "    phonemes = []\n",
    "    \n",
    "    wave = filtered_audio[1] # get audio wave\n",
    "    name = filtered_audio[0] # get the word and the person\n",
    "    name = name.split('/')[-2] # get the word said\n",
    "    phonetics = split_written_word(name) # split the word in syllables\n",
    "\n",
    "    while pos != len(splited_phonemes) - 2:\n",
    "        if pos == 0:\n",
    "            phoneme = phonetics[0]\n",
    "        else:\n",
    "            phoneme = phonetics[1]\n",
    "                \n",
    "        audio = wave[splited_phonemes[pos]:splited_phonemes[pos+1]] # get the wave per syllable\n",
    "        \n",
    "        audio_normalized = normalize_audio(audio, phoneme) # normalize audio\n",
    "        \n",
    "        audio_framed = framing_audio(audio_normalized, FFT_size=FFT_size, hop_size=hop_size, sample_rate=22050) # framming the audio\n",
    "        \n",
    "        audio_power = fft_spectrum(audio_framed, hop_size=hop_size, FFT_size=FFT_size) # perform fft and convert to power\n",
    "\n",
    "        freq_min, freq_max = maximum_minimum_hz(audio) # get the minimun and the maximum frequencies of the audio\n",
    "        \n",
    "        filters_to_mfcc =  get_filter_points(freq_min, freq_max, mel_filter_num=10, FFT_size=FFT_size, sample_rate=22050) # construct filter bank\n",
    "\n",
    "        predict = mfccs(filters_to_mfcc, audio_power, phoneme) # calculate the mfccs  \n",
    "\n",
    "        phonemes.append(predict)\n",
    "                \n",
    "        pos += 1\n",
    "        \n",
    "    return phonemes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65761a",
   "metadata": {},
   "source": [
    "# <center> <span style='color :blue' >Get the result from phoneme and the speaker recognition</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0380f6",
   "metadata": {},
   "source": [
    "# Let's test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f031af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(phoneme):\n",
    "    \n",
    "    model = pickle.load(open('final_model.sav', 'rb')) # load model\n",
    "\n",
    "    example = cv2.imread(phoneme +'.png', cv2.IMREAD_GRAYSCALE)\n",
    "    X_test = cv2.resize(example,[360,480])\n",
    "    \n",
    "    X_test = np.array(X_test).reshape(1,-1) # reshape data to the model\n",
    "    X_test = X_test/255.0 # normalize data\n",
    "    \n",
    "    prediction = model.predict(X_test) # test model\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2517317",
   "metadata": {},
   "source": [
    "### Calculate the fundamental frequency of each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_yin(y):\n",
    "    \n",
    "    frame_length = 2048\n",
    "    f0, voiced_flag, voiced_probs = pyin(y=y, fmin=80,fmax=500,frame_length=frame_length) # extract value from pyin\n",
    "    \n",
    "    times = times_like(f0)\n",
    "    D = amplitude_to_db(np.abs(stft(y)), ref=np.max)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    img = display.specshow(D, x_axis='time', y_axis='log', ax=ax)\n",
    "    ax.set(title='pYIN fundamental frequency estimation')\n",
    "    fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "    ax.plot(times, f0, label='f0', color='cyan', linewidth=4)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    f0 = np.mean(f0[np.where(voiced_flag == True)]) # calculate the mean of f0 values\n",
    "    \n",
    "    y_harmonic, y_percussive = effects.hpss(y) # extract percurssive and harmonics from the audio\n",
    "    y_harmonic = np.round(np.mean(y_harmonic), decimals=5) # calculate the mean of harmonics values\n",
    "    y_percussive = np.round(np.mean(y_percussive), decimals=6) # calculate the mean of percurssives values\n",
    "           \n",
    "    return f0, y_harmonic, y_percussive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500f4bd",
   "metadata": {},
   "source": [
    "### Get the result of speaker identification and the word that has been said"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1733bea7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_result():\n",
    "    predict= split_phoneme_to_test() # get the predictions of phonemes\n",
    "    \n",
    "    wave = filtered_audio[1] # get audio wave\n",
    "    f0, harmonic, percussive = p_yin(wave[audio_flag[0]:audio_flag[1]]) # analyze the audio where exists more text\n",
    "    print(f0, harmonic, percussive)\n",
    "    \n",
    "    phonemes = ['ca','cha', 'chu', 'da','far','la','pa', 'ri', 'sa', 'ta',\n",
    "               'va', 've']\n",
    "    \n",
    "    help_list = []\n",
    "    \n",
    "    x = 0\n",
    "    for i in range(len(phonemes)):\n",
    "        if x <2:\n",
    "            if predict[x] == phonemes[i]:\n",
    "                print(i)\n",
    "                help_list.append(i) \n",
    "                x += 1\n",
    "        else:\n",
    "            break\n",
    "     \n",
    "    if 140 <= f0 < 165 and harmonic == -0.0:\n",
    "        person = 'André'\n",
    "    elif 165 <= f0 < 193 and harmonic != -0.0:\n",
    "        person = 'Marcelo'\n",
    "    else:\n",
    "        person = 'Unknown'\n",
    "    \n",
    "    sentence = person+ ' said ' +phonemes[help_list[0]]+phonemes[help_list[1]]\n",
    "    print(sentence)#\n",
    "    \n",
    "    engine = pyttsx3.init() \n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', rate + 50)\n",
    "    engine.say(sentence) \n",
    "    engine.runAndWait()         \n",
    "    \n",
    "get_result()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2666e",
   "metadata": {},
   "source": [
    "# <center> <span style='color :blue' > And that is the end of the code !!!</span>\n",
    "## <center> <span style='color :blue' > Thank you !!!</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "600.844px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
